{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimisation for Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.neural_network import MLPClassifier, BernoulliRBM\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "    \n",
    "def read_data(filename):\n",
    "    return pd.read_csv(filename) \n",
    "\n",
    "class CommentVectorizer:\n",
    "    def __init__(self):\n",
    "        self._vectorizers = []\n",
    "        \n",
    "    def get_count_vectorizer(self, max_features = 1000, ngram_range = (1, 2), \n",
    "                             stop_words = 'english', binary = True):\n",
    "        \"\"\"\n",
    "        Initializes a count vectorizer with parameters set by the user and \n",
    "        returns an index in the internal vector array where the vectorizer\n",
    "        has been placed. We dont want any external entity manipulating the\n",
    "        vectorizer state directly.\n",
    "        \"\"\"\n",
    "        self._vectorizers.append(CountVectorizer(max_features = max_features, \n",
    "                                                 ngram_range = ngram_range, \n",
    "                                                 stop_words = stop_words,\n",
    "                                                 binary = binary))\n",
    "        return len(self._vectorizers) - 1\n",
    "    \n",
    "    def get_tdidf_vectorizer(self, max_features = 1000, use_idf = True):\n",
    "        self._vectorizers.append(TfidfVectorizer(max_df=0.5, max_features=max_features,\n",
    "                                 min_df=2, stop_words='english',\n",
    "                                 use_idf=use_idf))\n",
    "        return len(self._vectorizers) - 1\n",
    "    \n",
    "    def doc2vec_fit_transform(self, sentences):\n",
    "        \"\"\"\n",
    "        Method for building a doc2vec model. Unfortunately it does not follow the nice fit/transform\n",
    "        pattern of the scikit models.\n",
    "        \"\"\"\n",
    "        documents = map(lambda i: TaggedDocument(sentences[i].split(), [i]),\n",
    "                        range(len(sentences)))\n",
    "        self._doc2vec_model = Doc2Vec(documents, size=100, window=8, min_count=0, workers=8)\n",
    "        return map(lambda x: self._doc2vec_model.docvecs[x], range(len(sentences)))\n",
    "    \n",
    "    def doc2vec_transform(self, sentences):\n",
    "        vectors = []\n",
    "        for i in range(len(sentences)):\n",
    "            vectors.append(self._doc2vec_model.infer_vector(sentences[i].split()))\n",
    "        return vectors\n",
    "    \n",
    "    def _exists(self, vectorizer):\n",
    "        \"\"\"\n",
    "        Checks if the vectorizer index provided points to a valid vectorizer.\n",
    "        \"\"\"\n",
    "        if vectorizer < 0 or len(self._vectorizers) <= vectorizer:\n",
    "            raise Exception('Vectorizer index out of bound.')\n",
    "            \n",
    "        if self._vectorizers[vectorizer] == None:\n",
    "            raise Exception('Vectorizer not initialized.')\n",
    "            \n",
    "        pass\n",
    "        \n",
    "    def fit(self, comments = [], vectorizer = -1):\n",
    "        self._exists(vectorizer)\n",
    "        self._vectorizers[vectorizer].fit(comments)\n",
    "        \n",
    "    def transform(self, comments, vectorizer):\n",
    "        self._exists(vectorizer)\n",
    "        return self._vectorizers[vectorizer].transform(comments)  \n",
    "\n",
    "class ExtendedMultiOutputClassifier(MultiOutputClassifier):\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Add a transform method to the classifier because it is mandatory for steps of a pipeline\n",
    "        to provide fit and transform methods.\n",
    "        \"\"\"\n",
    "        _o = self.predict_proba(X)\n",
    "        return np.concatenate(_o, axis=1)\n",
    "    \n",
    "def multi_roc_auc(ground_truth, predictions):\n",
    "    roc_aucs = []\n",
    "    gt = ground_truth.as_matrix()\n",
    "    for col_index in range(ground_truth.shape[1]):\n",
    "        g_c = gt[:, col_index]\n",
    "        p_c = predictions[:, col_index]\n",
    "        roc_aucs.append(metrics.roc_auc_score(g_c, p_c))\n",
    "    return np.mean(roc_aucs)\n",
    "custom_scorer = make_scorer(multi_roc_auc, greater_is_better=True)\n",
    "\n",
    "if __name__ == '__main__'    :\n",
    "    # path to the train data file\n",
    "    INDATA_LOCATION = 'C:/Users/sharm/Desktop/Dat5Melb/Final_Project/Datasets/train/train.csv'\n",
    "    \n",
    "    # utility definitions for easier handling of the dataset column names\n",
    "    TEXT_COLUMN = 'comment_text'\n",
    "    CLASS_TOXIC, CLASS_SEVER_TOXIC, CLASS_OBSCENE, CLASS_THREAT, CLASS_INSULT, \\\n",
    "        CLASS_IDENTITY_HATE = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \\\n",
    "                               \"insult\", \"identity_hate\"]\n",
    "    CLASSES = [CLASS_TOXIC, CLASS_SEVER_TOXIC, CLASS_OBSCENE, CLASS_THREAT, CLASS_INSULT, CLASS_IDENTITY_HATE]\n",
    "    \n",
    "    # read the comments and associated classification data \n",
    "    dataDf = read_data(INDATA_LOCATION)\n",
    "    \n",
    "    # initialize a count vectorizer for this experiment    \n",
    "    commentVectorizer = CommentVectorizer()\n",
    "    #commentVectors = np.array(commentVectorizer.doc2vec_fit_transform(dataDf[TEXT_COLUMN]))\n",
    "    vectorizer = commentVectorizer.get_tdidf_vectorizer()\n",
    "    commentVectorizer.fit(dataDf[TEXT_COLUMN], vectorizer)\n",
    "    commentVectors = commentVectorizer.transform(dataDf[TEXT_COLUMN], vectorizer)\n",
    "    \n",
    "    # set this true to evaluate combination of parameters\n",
    "    if True:\n",
    "        print(\"# Tuning hyper-parameters for custom_scorer\")\n",
    "        print()\n",
    "    \n",
    "        moc = ExtendedMultiOutputClassifier(RandomForestClassifier(n_estimators = 50, class_weight = 'balanced', n_jobs=-1, criterion=\"entropy\", oob_score=True))\n",
    "        nnc = MLPClassifier(solver='sgd', random_state=1)\n",
    "        pipeline = Pipeline([('moc', moc), ('nnc', nnc)])\n",
    "    \n",
    "        # note unfortunately since the actual classifier is wrapped inside the\n",
    "        # multioutputclassifier there is not _easy_ of tuning the parameters\n",
    "        # of that classifier. maybe it should be done separately\n",
    "        param_grid = dict(nnc__alpha=[.0001, .00001, .000001], nnc__hidden_layer_sizes=range(15, 50, 10), nnc__activation=['identity', 'tanh', 'logistic', 'relu'], nnc__learning_rate=['constant', 'invscaling', 'adaptive'], nnc__momentum=[0.9,0.99,0.999], nnc__tol=[.0001, .00001, .000001])\n",
    "    \n",
    "        clf = GridSearchCV(pipeline, param_grid=param_grid, cv=5, scoring=custom_scorer)\n",
    "        clf.fit(commentVectors, dataDf[CLASSES])\n",
    "    \n",
    "        print(\"Best parameters set found on development set:\")\n",
    "        print()\n",
    "        print(clf.best_params_)\n",
    "        print()\n",
    "        print(\"Grid scores on development set:\")\n",
    "        print()\n",
    "        means = clf.cv_results_['mean_test_score']\n",
    "        stds = clf.cv_results_['std_test_score']\n",
    "        for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "            print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "                  % (mean, std * 2, params))\n",
    "        print()    \n",
    "    \n",
    "    # set the desired parameters here for generating predictions on test set\n",
    "    if False:\n",
    "        moc = ExtendedMultiOutputClassifier(RandomForestClassifier(n_estimators = 20, class_weight = 'balanced'))\n",
    "        nnc = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(15,), random_state=1)\n",
    "        # specify the order in which pipeline should execute the classifiers/estimators\n",
    "        clf = Pipeline([('emc', moc), ('nnc', nnc)])\n",
    "        # fit all the transforms one after the other and transform the data, then fit the transformed data using the final estimator.\n",
    "        clf.fit(commentVectors, dataDf[CLASSES])\n",
    "        \n",
    "        # test data\n",
    "        testdf = pd.read_csv('C:/Users/sharm/Desktop/Dat5Melb/Final_Project/Datasets/test1/test.csv')\n",
    "        testdf.head()\n",
    "        # embed comments into vector space\n",
    "        testCommentVectors = commentVectorizer.transform(testdf[TEXT_COLUMN], vectorizer)\n",
    "        #commentVectors\n",
    "        #testCommentVectors = np.array(commentVectorizer.doc2vec_transform(testdf[TEXT_COLUMN]))\n",
    "        testpredictions = clf.predict_proba(testCommentVectors)\n",
    "        testpdf = pd.DataFrame(data=testpredictions)\n",
    "        submissiondf = testpdf.join(testdf['id'], how='left')\n",
    "        submissiondf = submissiondf[['id',0,1,2,3,4,5]]\n",
    "        submissiondf.columns = [\"id\", \"toxic\", \"severe_toxic\", \"obscene\", \"threat\",\"insult\", \"identity_hate\"]\n",
    "        submissiondf.head()\n",
    "        submissiondf.to_csv('C:/Users/sharm/Desktop/Dat5Melb/Final_Project/submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda2]",
   "language": "python",
   "name": "conda-env-Anaconda2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
