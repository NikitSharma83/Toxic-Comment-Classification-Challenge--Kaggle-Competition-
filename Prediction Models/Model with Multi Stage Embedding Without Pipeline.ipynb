{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.neural_network import MLPClassifier, BernoulliRBM\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.svm import SVC, SVR, LinearSVR\n",
    "from sklearn import linear_model as sklean_lms\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    return pd.read_csv(filename) \n",
    "\n",
    "def get_tfidf_vectorizer(sentences):\n",
    "    _m = TfidfVectorizer(max_df=0.5, max_features = 5000,\n",
    "                                 min_df=2, stop_words='english',\n",
    "                                 use_idf=True)\n",
    "    _m.fit(sentences)\n",
    "    return _m\n",
    "\n",
    "def get_range_tfidf_vectorizer(sentences):\n",
    "    _m = TfidfVectorizer(max_df=0.5, ngram_range=(2,3), max_features = 5000,\n",
    "                                 min_df=1, stop_words='english',\n",
    "                                 use_idf=True)\n",
    "    _m.fit(sentences)\n",
    "    return _m\n",
    "\n",
    "def get_vectors(m1, m2, sentences):\n",
    "    x1 = m1.transform(sentences)\n",
    "    x2 = m2.transform(sentences)\n",
    "    return sp.hstack([x1, x2])\n",
    "\n",
    "class ExtendedMultiOutputClassifier(MultiOutputClassifier):\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Add a transform method to the classifier because it is mandatory for steps of a pipeline\n",
    "        to provide fit and transform methods.\n",
    "        \"\"\"\n",
    "        # for RF\n",
    "        _o = self.predict_proba(X)\n",
    "        return np.concatenate(_o, axis=1)\n",
    "        # for SVM\n",
    "        #return self.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(156658, 8)\n"
     ]
    }
   ],
   "source": [
    "INDATA_LOCATION = 'C:/Users/sharm/Desktop/Dat5Melb/Final_Project/Datasets/train/train_new.csv'\n",
    "\n",
    "# utility definitions for easier handling of the dataset column names\n",
    "TEXT_COLUMN = 'comment_text'\n",
    "CLASS_TOXIC, CLASS_SEVER_TOXIC, CLASS_OBSCENE, CLASS_THREAT, CLASS_INSULT, \\\n",
    "    CLASS_IDENTITY_HATE = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \\\n",
    "                           \"insult\", \"identity_hate\"]\n",
    "CLASSES = [CLASS_TOXIC, CLASS_SEVER_TOXIC, CLASS_OBSCENE, CLASS_THREAT, CLASS_INSULT, CLASS_IDENTITY_HATE]\n",
    "\n",
    "# read the comments and associated classification data \n",
    "dataDf = read_data(INDATA_LOCATION)\n",
    "print(dataDf.shape)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tBuilding model\n",
      "\tMaking vectors\n"
     ]
    }
   ],
   "source": [
    "perf = []\n",
    "\n",
    "## shuffle and split the dataset stratified by the number of classifications of a data point\n",
    "## for balancing across resulting modeling and evaluation datasets\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.8, random_state=0)\n",
    "for train_index, test_index in sss.split(np.zeros(len(dataDf)), dataDf[CLASSES].sum(axis=1)):\n",
    "    pass\n",
    "dataDf_modeling = dataDf.iloc[train_index]\n",
    "dataDf_testing = dataDf.iloc[test_index]\n",
    "\n",
    "## lets create one doc2vec model from everything\n",
    "print('\\tBuilding model')\n",
    "model1 = get_tfidf_vectorizer(dataDf_modeling[TEXT_COLUMN])\n",
    "model2 = get_range_tfidf_vectorizer(dataDf_modeling[TEXT_COLUMN])\n",
    "\n",
    "## lets prepare the data vectors\n",
    "#X_orig = model.transform(dataDf_modeling[TEXT_COLUMN])\n",
    "## lets reduce the dims\n",
    "print('\\tMaking vectors')\n",
    "X = get_vectors(model1, model2, dataDf_modeling[TEXT_COLUMN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tBuilding RFs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ExtendedMultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=8,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "               n_jobs=1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## learn RFs\n",
    "print('\\tBuilding RFs')\n",
    "moc = ExtendedMultiOutputClassifier(RandomForestClassifier(n_estimators = 100, n_jobs=8))\n",
    "#moc = ExtendedMultiOutputClassifier(SVC(kernel='linear', probability=True))\n",
    "moc.fit(X, dataDf_modeling[CLASSES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tBuilding NN\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', alpha=1e-06, batch_size='auto',\n",
       "       beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100, 100), learning_rate='adaptive',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
       "       solver='sgd', tol=1e-15, validation_fraction=0.1, verbose=False,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## learn MLP\n",
    "print('\\tBuilding NN')\n",
    "mlp = MLPClassifier(solver='sgd', activation='logistic', learning_rate='adaptive', momentum=0.9, alpha=1e-6, hidden_layer_sizes=(100, 100), random_state=1, tol=1e-15)\n",
    "mlp.fit(moc.transform(X), dataDf_modeling[CLASSES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tPrepare test vectors\n",
      "\tPredicting classes\n",
      "\tEvaluating\n",
      "\tMean AUC: 0.926226\n",
      "MSE: 0.075917\n"
     ]
    }
   ],
   "source": [
    "## predict on test set\n",
    "print('\\tPrepare test vectors')\n",
    "X_test = get_vectors(model1, model2, dataDf_testing[TEXT_COLUMN])\n",
    "\n",
    "print('\\tPredicting classes')\n",
    "predicted = mlp.predict_proba(moc.transform(X_test))\n",
    "\n",
    "predictedDf = pd.DataFrame(predicted)\n",
    "predictedDf.columns = CLASSES\n",
    "\n",
    "print('\\tEvaluating')\n",
    "# mean auc\n",
    "aucs = map(lambda klass: metrics.roc_auc_score(dataDf_testing[klass], predictedDf[klass]), CLASSES)\n",
    "# MSE\n",
    "d = predicted - dataDf_testing[CLASSES]\n",
    "sq_difs = map(lambda x: np.dot(x, x.T), d.as_matrix())\n",
    "\n",
    "print('\\tMean AUC: %f' %np.mean(aucs))\n",
    "print('MSE: %f' %(np.sum(sq_difs) * 1.0 / len(d)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if False:\n",
    "TESTDATA_LOCATION = 'C:/Users/sharm/Desktop/Dat5Melb/Final_Project/Datasets/test1/test.csv'\n",
    "testDf = pd.read_csv(TESTDATA_LOCATION)\n",
    "\n",
    "X_sub = get_vectors(model1, model2, testDf[TEXT_COLUMN])\n",
    "y_sub = pd.DataFrame(mlp.predict_proba(moc.transform(X_sub)))\n",
    "y_sub.columns = CLASSES\n",
    "\n",
    "subDf = pd.concat([testDf['id'], y_sub], axis=1)\n",
    "\n",
    "subDf.head()\n",
    "subDf.to_csv('C:/Users/sharm/Desktop/Dat5Melb/Final_Project/submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda2]",
   "language": "python",
   "name": "conda-env-Anaconda2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
