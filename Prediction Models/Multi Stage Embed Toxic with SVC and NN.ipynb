{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sharm\\Anaconda2\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\sharm\\Anaconda2\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.neural_network import MLPClassifier, BernoulliRBM\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.svm import SVC, SVR, LinearSVR\n",
    "from sklearn import linear_model as sklean_lms\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    return pd.read_csv(filename) \n",
    "\n",
    "def get_tfidf_vectorizer(sentences):\n",
    "    _m = TfidfVectorizer(max_df=0.5, max_features = 5000,\n",
    "                                 min_df=2, stop_words='english',\n",
    "                                 use_idf=True)\n",
    "    _m.fit(sentences)\n",
    "    return _m\n",
    "\n",
    "def get_range_tfidf_vectorizer(sentences):\n",
    "    _m = TfidfVectorizer(max_df=0.5, ngram_range=(2,3), max_features = 5000,\n",
    "                                 min_df=1, stop_words='english',\n",
    "                                 use_idf=True)\n",
    "    _m.fit(sentences)\n",
    "    return _m\n",
    "\n",
    "def get_vectors(m1, m2, sentences):\n",
    "    x1 = m1.transform(sentences)\n",
    "    x2 = m2.transform(sentences)\n",
    "    return sp.hstack([x1, x2])\n",
    "\n",
    "class ExtendedMultiOutputClassifier(MultiOutputClassifier):\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Add a transform method to the classifier because it is mandatory for steps of a pipeline\n",
    "        to provide fit and transform methods.\n",
    "        \"\"\"\n",
    "        # for RF\n",
    "        _o = self.predict_proba(X)\n",
    "        return np.concatenate(_o, axis=1)\n",
    "        # for SVM\n",
    "        #return self.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 8)\n"
     ]
    }
   ],
   "source": [
    "INDATA_LOCATION = 'C:/Users/sharm/Desktop/Dat5Melb/Final_Project/Datasets/train/train.csv'\n",
    "\n",
    "# utility definitions for easier handling of the dataset column names\n",
    "TEXT_COLUMN = 'comment_text'\n",
    "CLASS_TOXIC, CLASS_SEVER_TOXIC, CLASS_OBSCENE, CLASS_THREAT, CLASS_INSULT, \\\n",
    "    CLASS_IDENTITY_HATE = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \\\n",
    "                           \"insult\", \"identity_hate\"]\n",
    "CLASSES = [CLASS_TOXIC, CLASS_SEVER_TOXIC, CLASS_OBSCENE, CLASS_THREAT, CLASS_INSULT, CLASS_IDENTITY_HATE]\n",
    "\n",
    "# read the comments and associated classification data \n",
    "dataDf = read_data(INDATA_LOCATION)\n",
    "print(dataDf.shape)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tBuilding model\n",
      "\tMaking vectors\n"
     ]
    }
   ],
   "source": [
    "perf = []\n",
    "\n",
    "## shuffle and split the dataset stratified by the number of classifications of a data point\n",
    "## for balancing across resulting modeling and evaluation datasets\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.8, random_state=0)\n",
    "for train_index, test_index in sss.split(np.zeros(len(dataDf)), dataDf[CLASSES].sum(axis=1)):\n",
    "    pass\n",
    "dataDf_modeling = dataDf.iloc[train_index]\n",
    "dataDf_testing = dataDf.iloc[test_index]\n",
    "\n",
    "## lets create one doc2vec model from everything\n",
    "print('\\tBuilding model')\n",
    "model1 = get_tfidf_vectorizer(dataDf_modeling[TEXT_COLUMN])\n",
    "model2 = get_range_tfidf_vectorizer(dataDf_modeling[TEXT_COLUMN])\n",
    "\n",
    "## lets prepare the data vectors\n",
    "#X_orig = model.transform(dataDf_modeling[TEXT_COLUMN])\n",
    "## lets reduce the dims\n",
    "print('\\tMaking vectors')\n",
    "X = get_vectors(model1, model2, dataDf_modeling[TEXT_COLUMN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tBuilding SVRs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ExtendedMultiOutputClassifier(estimator=SVR(C=1e-06, cache_size=50, coef0=0.0, degree=3, epsilon=0.001, gamma='auto',\n",
       "  kernel='linear', max_iter=-1, shrinking=True, tol=0.001, verbose=False),\n",
       "               n_jobs=1)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## learn SVCs\n",
    "print('\\tBuilding SVRs')\n",
    "moc = ExtendedMultiOutputClassifier(SVR(C=1e-6, cache_size=50, decision_function_shape='ovo', probability=True, kernel='linear', max_iter=-1, random_state=1, tol=0.001))\n",
    "moc.fit(X, dataDf_modeling[CLASSES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31914, 8)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataDf_modeling.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## learn MLP\n",
    "print('\\tBuilding NN')\n",
    "mlp = MLPClassifier(solver='sgd', activation='logistic', learning_rate='adaptive', max_iter=1000, momentum=0.9, alpha=1e-8, hidden_layer_sizes=(100, 100), random_state=1, tol=1e-15)\n",
    "mlp.fit(moc.transform(X), dataDf_modeling[CLASSES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## predict on test set\n",
    "print('\\tPrepare test vectors')\n",
    "X_test = get_vectors(model1, model2, dataDf_testing[TEXT_COLUMN])\n",
    "\n",
    "print('\\tPredicting classes')\n",
    "predicted = mlp.predict_proba(moc.transform(X_test))\n",
    "\n",
    "predictedDf = pd.DataFrame(predicted)\n",
    "predictedDf.columns = CLASSES\n",
    "\n",
    "print('\\tEvaluating')\n",
    "# mean auc\n",
    "aucs = map(lambda klass: metrics.roc_auc_score(dataDf_testing[klass], predictedDf[klass]), CLASSES)\n",
    "# MSE\n",
    "d = predicted - dataDf_testing[CLASSES]\n",
    "sq_difs = map(lambda x: np.dot(x, x.T), d.as_matrix())\n",
    "\n",
    "print('\\tMean AUC: %f' %np.mean(aucs))\n",
    "print('MSE: %f' %(np.sum(sq_difs) * 1.0 / len(d)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if False:\n",
    "TESTDATA_LOCATION = 'C:/Users/sharm/Desktop/Dat5Melb/Final_Project/Datasets/test1/test.csv'\n",
    "testDf = pd.read_csv(TESTDATA_LOCATION)\n",
    "\n",
    "X_sub = get_vectors(model1, model2, testDf[TEXT_COLUMN])\n",
    "y_sub = pd.DataFrame(mlp.predict_proba(moc.transform(X_sub)))\n",
    "y_sub.columns = CLASSES\n",
    "\n",
    "subDf = pd.concat([testDf['id'], y_sub], axis=1)\n",
    "\n",
    "subDf.head()\n",
    "subDf.to_csv('C:/Users/sharm/Desktop/Dat5Melb/Final_Project/submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda2]",
   "language": "python",
   "name": "conda-env-Anaconda2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
