{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports for this Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.neural_network import MLPClassifier, BernoulliRBM\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path to the train data file\n",
    "INDATA_LOCATION = 'C:/Users/sharm/Desktop/Dat5Melb/Final_Project/Datasets/train/train.csv'\n",
    "\n",
    "# utility definitions for easier handling of the dataset column names\n",
    "TEXT_COLUMN = 'comment_text'\n",
    "CLASS_TOXIC, CLASS_SEVER_TOXIC, CLASS_OBSCENE, CLASS_THREAT, CLASS_INSULT, \\\n",
    "    CLASS_IDENTITY_HATE = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \\\n",
    "                           \"insult\", \"identity_hate\"]\n",
    "CLASSES = [CLASS_TOXIC, CLASS_SEVER_TOXIC, CLASS_OBSCENE, CLASS_THREAT, CLASS_INSULT, CLASS_IDENTITY_HATE]\n",
    "\n",
    "def read_data(filename):\n",
    "    return pd.read_csv(filename) \n",
    "\n",
    "# read the comments and associated classification data \n",
    "dataDf = read_data(INDATA_LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic data characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_characteristics(df):\n",
    "    print('Number of data points: %d' %len(df))\n",
    "    for klass in CLASSES:\n",
    "        print('Number data points of type %s: %d' %(klass, len(df[df[klass]==1])))        \n",
    "basic_characteristics(dataDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, counts = np.unique(dataDf[CLASSES].sum(axis=1), return_counts=True)\n",
    "plt.bar(labels, counts, align='center')\n",
    "plt.gca().set_title('Histogram of number of classes per datapoint')\n",
    "plt.gca().set_xlabel('Number of classes per datapoint')\n",
    "plt.gca().set_xticks(labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed text in vector space\n",
    "\n",
    "We use a simple count based vectorizer to embed the comment text into vector space in preparation for building classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class CommentVectorizer:\n",
    "#     def __init__(self):\n",
    "#         self._vectorizers = []\n",
    "        \n",
    "#     def get_count_vectorizer(self, max_features = 1000, ngram_range = (1, 2), \n",
    "#                              stop_words = 'english', binary = True):\n",
    "#         \"\"\"\n",
    "#         Initializes a count vectorizer with parameters set by the user and \n",
    "#         returns an index in the internal vector array where the vectorizer\n",
    "#         has been placed. We dont want any external entity manipulating the\n",
    "#         vectorizer state directly.\n",
    "#         \"\"\"\n",
    "#         self._vectorizers.append(CountVectorizer(max_features = max_features, \n",
    "#                                                  ngram_range = ngram_range, \n",
    "#                                                  stop_words = stop_words,\n",
    "#                                                  binary = binary))\n",
    "#         return len(self._vectorizers) - 1\n",
    "    \n",
    "#     def get_tdidf_vectorizer(self, max_features = 5000, use_idf = True):\n",
    "#         self._vectorizers.append(TfidfVectorizer(max_df=0.7, max_features=max_features,\n",
    "#                                  min_df=2, stop_words='english',\n",
    "#                                  use_idf=use_idf))\n",
    "#         return len(self._vectorizers) - 1\n",
    "    \n",
    "#     def doc2vec_fit_transform(self, sentences):\n",
    "#         \"\"\"\n",
    "#         Method for building a doc2vec model. Unfortunately it does not follow the nice fit/transform\n",
    "#         pattern of the scikit models.\n",
    "#         \"\"\"\n",
    "#         documents = map(lambda i: TaggedDocument(sentences[i].split(), [i]),\n",
    "#                         range(len(sentences)))\n",
    "#         model = Doc2Vec(documents, size=100, window=8, min_count=0, workers=8)\n",
    "#         return map(lambda x: model.docvecs[x], range(len(sentences)))\n",
    "    \n",
    "#     def _exists(self, vectorizer):\n",
    "#         \"\"\"\n",
    "#         Checks if the vectorizer index provided points to a valid vectorizer.\n",
    "#         \"\"\"\n",
    "#         if vectorizer < 0 or len(self._vectorizers) <= vectorizer:\n",
    "#             raise Exception('Vectorizer index out of bound.')\n",
    "            \n",
    "#         if self._vectorizers[vectorizer] == None:\n",
    "#             raise Exception('Vectorizer not initialized.')\n",
    "            \n",
    "#         pass\n",
    "        \n",
    "#     def fit(self, comments = [], vectorizer = -1):\n",
    "#         self._exists(vectorizer)\n",
    "#         self._vectorizers[vectorizer].fit(comments)\n",
    "        \n",
    "#     def transform(self, comments, vectorizer):\n",
    "#         self._exists(vectorizer)\n",
    "#         return self._vectorizers[vectorizer].transform(comments)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tfidf_vectorizer(sentences):\n",
    "    _m = TfidfVectorizer(max_df=0.5, max_features = 5000,\n",
    "                                 min_df=2, stop_words='english',\n",
    "                                 use_idf=True)\n",
    "    _m.fit(sentences)\n",
    "    return _m\n",
    "\n",
    "def get_range_tfidf_vectorizer(sentences):\n",
    "    _m = TfidfVectorizer(max_df=0.5, ngram_range=(2,3), max_features = 5000,\n",
    "                                 min_df=1, stop_words='english',\n",
    "                                 use_idf=True)\n",
    "    _m.fit(sentences)\n",
    "    return _m\n",
    "\n",
    "def get_vectors(m1, m2, sentences):\n",
    "    x1 = m1.transform(sentences)\n",
    "    x2 = m2.transform(sentences)\n",
    "    return sp.hstack([x1, x2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize a count vectorizer for this experiment    \n",
    "# commentVectorizer = CommentVectorizer()\n",
    "# vectorizer = commentVectorizer.get_tdidf_vectorizer()\n",
    "# commentVectorizer.fit(dataDf[TEXT_COLUMN], vectorizer)\n",
    "\n",
    "# embed comments into vector space\n",
    "# commentVectors = commentVectorizer.transform(dataDf[TEXT_COLUMN], vectorizer)\n",
    "# commentVectors = np.array(commentVectorizer.doc2vec_fit_transform(dataDf[TEXT_COLUMN]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coords = TruncatedSVD(n_components=2).fit_transform(commentVectors)\n",
    "# plt.scatter(coords[:,0], coords[:,1], color='red' )\n",
    "# plt.title('Scatter plot of the comment vectors (reduced)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create modeling and evaluation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle and split the dataset stratified by the number of classifications of a data point\n",
    "# for balancing across resulting modeling and evaluation datasets\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
    "for train_index, test_index in sss.split(np.zeros(len(dataDf)), dataDf[CLASSES].sum(axis=1)):\n",
    "    pass\n",
    "\n",
    "dataDf_modeling = dataDf.iloc[train_index]\n",
    "dataDf_testing = dataDf.iloc[test_index]\n",
    "\n",
    "## lets create one doc2vec model from everything\n",
    "print('\\tBuilding model')\n",
    "model1 = get_tfidf_vectorizer(dataDf_modeling[TEXT_COLUMN])\n",
    "model2 = get_range_tfidf_vectorizer(dataDf_modeling[TEXT_COLUMN])\n",
    "\n",
    "## lets prepare the data vectors\n",
    "#X_orig = model.transform(dataDf_modeling[TEXT_COLUMN])\n",
    "## lets reduce the dims\n",
    "print('\\tMaking vectors')\n",
    "X = get_vectors(model1, model2, dataDf_modeling[TEXT_COLUMN])\n",
    "\n",
    "# # modeling dataset\n",
    "# modeling_vectors = commentVectors[train_index]\n",
    "# modeling_classes = dataDf[CLASSES].loc[train_index]\n",
    "# # print('Modeling data size: %d' %len(modeling_classes))\n",
    "# # basic_characteristics(modeling_classes)\n",
    "\n",
    "\n",
    "# # evaluation dataset\n",
    "# evaluation_vectors = commentVectors[test_index]\n",
    "# evaluation_classes = dataDf[CLASSES].loc[test_index]\n",
    "# # print('Evaluation data size: %d' %len(evaluation_classes))\n",
    "# # basic_characteristics(evaluation_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = modeling_vectors\n",
    "y = modeling_classes\n",
    "\n",
    "\n",
    "rfc = RandomForestClassifier(n_jobs=-1,max_features= 'sqrt' ,n_estimators=50, oob_score = True) \n",
    "\n",
    "param_grid = { \n",
    "    'n_estimators': [5, 10],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)\n",
    "CV_rfc.fit(X, y)\n",
    "print CV_rfc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SVC(C=1.0, cache_size=50, class_weight='balanced', decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf', max_iter=-1, probability=True, random_state=1, shrinking=True, tol=0.001)\n",
    "RandomForestClassifier(n_estimators = 150, class_weight = 'balanced', n_jobs=-1, criterion=\"entropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "class ExtendedMultiOutputClassifier(MultiOutputClassifier):\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Add a transform method to the classifier because it is mandatory for steps of a pipeline\n",
    "        to provide fit and transform methods.\n",
    "        \"\"\"\n",
    "        _o = self.predict_proba(X)\n",
    "        return np.concatenate(_o, axis=1)\n",
    "\n",
    "moc = ExtendedMultiOutputClassifier(SVC(C=1e-3, cache_size=10, decision_function_shape='ovo', kernel='linear', max_iter=7000, probability=True, random_state=1, tol=0.001))\n",
    "nnc = MLPClassifier(solver='sgd', activation='logistic', learning_rate='adaptive', momentum=0.9, alpha=1e-2, hidden_layer_sizes=(150, 150), random_state=1, tol=1e-15)\n",
    "# specify the order in which pipeline should execute the classifiers/estimators\n",
    "clf = Pipeline([('moc_rf', moc), ('nnc', nnc)])\n",
    "# fit all the transforms one after the other and transform the data, then fit the transformed data using the final estimator.\n",
    "clf.fit(X, dataDf_modeling[CLASSES])\n",
    "# rudimentary test\n",
    "predictions = clf.predict_proba(X)\n",
    "\n",
    "# calculate MSE (mean squared error). note np.dot on full matrix gives \n",
    "# memmory error hence the slow work around\n",
    "d = predictions - dataDf_modeling[CLASSES]\n",
    "sq_difs = map(lambda x: np.dot(x, x.T), d.as_matrix())\n",
    "print('MSE: %f' %(np.sum(sq_difs) * 1.0 / len(d))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## predict on test set\n",
    "print('\\tPrepare test vectors')\n",
    "X_test = get_vectors(model1, model2, dataDf_testing[TEXT_COLUMN])\n",
    "\n",
    "print('\\tPredicting classes')\n",
    "predicted = mlp.predict_proba(moc.transform(X_test))\n",
    "\n",
    "predictedDf = pd.DataFrame(predicted)\n",
    "predictedDf.columns = CLASSES\n",
    "\n",
    "print('\\tEvaluating')\n",
    "# mean auc\n",
    "aucs = map(lambda klass: metrics.roc_auc_score(dataDf_testing[klass], predictedDf[klass]), CLASSES)\n",
    "# MSE\n",
    "d = predicted - dataDf_testing[CLASSES]\n",
    "sq_difs = map(lambda x: np.dot(x, x.T), d.as_matrix())\n",
    "\n",
    "print('\\tMean AUC: %f' %np.mean(aucs))\n",
    "print('MSE: %f' %(np.sum(sq_difs) * 1.0 / len(d)))\n",
    "\n",
    "# predictions = clf.predict_proba(evaluation_vectors)\n",
    "\n",
    "# # MSE\n",
    "# d = predictions - evaluation_classes\n",
    "# sq_difs = map(lambda x: np.dot(x, x.T), d.as_matrix())\n",
    "# print('MSE: %f' %(np.sum(sq_difs) * 1.0 / len(d)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testdf = pd.read_csv('C:/Users/sharm/Desktop/Dat5Melb/Final_Project/Datasets/test1/test.csv')\n",
    "testdf.id = testdf.id.astype(basestring)\n",
    "testdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testdf.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embded test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# embed comments into vector space\n",
    "testcommentVectors = commentVectorizer.transform(testdf[TEXT_COLUMN], vectorizer)\n",
    "testcommentVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict final probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testpredictions = clf.predict_proba(testcommentVectors)\n",
    "testpdf = pd.DataFrame(data=testpredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submissiondf = testpdf.join(testdf['id'], how='left')\n",
    "submissiondf = submissiondf[['id',0,1,2,3,4,5]]\n",
    "submissiondf.columns = [\"id\", \"toxic\", \"severe_toxic\", \"obscene\", \"threat\",\"insult\", \"identity_hate\"]\n",
    "submissiondf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# submissiondf.to_csv('submission.csv', index=False)\n",
    "\n",
    "def manual_write(submissiondf,filename):\n",
    "    of = file(filename, 'w')\n",
    "    of.write('%s\\n' %','.join(submissiondf.columns))\n",
    "    for idx in range(len(submissiondf)):\n",
    "        of.write('%s\\n' %','.join(map(str, submissiondf.iloc[idx].tolist())))\n",
    "    of.close()\n",
    "\n",
    "manual_write(submissiondf, 'submission2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submissiondf.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda2]",
   "language": "python",
   "name": "conda-env-Anaconda2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
