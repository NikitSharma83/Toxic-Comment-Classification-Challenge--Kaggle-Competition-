{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asharma/anaconda2-5/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.neural_network import MLPClassifier, BernoulliRBM\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.svm import SVC, SVR, LinearSVR\n",
    "from sklearn import linear_model as sklean_lms\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# conda install -c glemaitre imbalanced-learn\n",
    "from imblearn.over_sampling import SMOTE, ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELING_DATA_LOCATION = '/home/asharma/data/toxic_challenge/train.csv'\n",
    "SUBMISSION_DATA_LOCATION = '/home/asharma/data/toxic_challenge/test.csv'\n",
    "\n",
    "# utility definitions for easier handling of the dataset column names\n",
    "TEXT_COLUMN = 'comment_text'\n",
    "CLASS_TOXIC, CLASS_SEVER_TOXIC, CLASS_OBSCENE, CLASS_THREAT, CLASS_INSULT, \\\n",
    "    CLASS_IDENTITY_HATE = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \\\n",
    "                           \"insult\", \"identity_hate\"]\n",
    "CLASSES = [CLASS_TOXIC, CLASS_SEVER_TOXIC, CLASS_OBSCENE, CLASS_THREAT, CLASS_INSULT, CLASS_IDENTITY_HATE]\n",
    "\n",
    "# read the modeling data \n",
    "modelingDataDf = pd.read_csv(MODELING_DATA_LOCATION)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make training and test sets\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.35, random_state=0)\n",
    "for train_index, test_index in sss.split(np.zeros(len(modelingDataDf)), modelingDataDf[CLASSES].sum(axis=1)):\n",
    "    pass\n",
    "\n",
    "trainingDataDf = modelingDataDf.iloc[train_index]\n",
    "testDataDf = modelingDataDf.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get embedding vectors\n",
    "vectorizer = TfidfVectorizer(max_df=0.8, max_features=5000,\n",
    "                                 min_df=2, stop_words='english',\n",
    "                                 use_idf=True, ngram_range=(1,3), smooth_idf=True)\n",
    "\n",
    "X_training = vectorizer.fit_transform(trainingDataDf[TEXT_COLUMN])\n",
    "X_testing = vectorizer.transform(testDataDf[TEXT_COLUMN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing toxic\n",
      "Rf score 0.995863\n",
      "Processing severe_toxic\n",
      "Rf score 0.996431\n",
      "Processing obscene\n",
      "Rf score 0.996293\n",
      "Processing threat\n",
      "Rf score 0.996881\n",
      "Processing insult\n",
      "Rf score 0.995984\n",
      "Processing identity_hate\n",
      "Rf score 0.996694\n"
     ]
    }
   ],
   "source": [
    "# build RFs\n",
    "rf_models = []\n",
    "\n",
    "for klass in CLASSES:\n",
    "    print('Processing %s' %klass)\n",
    "    rf = RandomForestClassifier(n_estimators = 100, n_jobs=-1, criterion=\"entropy\", oob_score=True, verbose=0)\n",
    "    X_resampled, y_resampled = SMOTE().fit_sample(X_training, trainingDataDf[klass])\n",
    "    rf.fit(X_resampled, y_resampled)\n",
    "    print('Rf score %f' %rf.score(X_resampled, y_resampled))\n",
    "    rf_models.append(rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asharma/anaconda2-5/lib/python2.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', alpha=1e-06, batch_size='auto',\n",
       "       beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100, 100), learning_rate='adaptive',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
       "       solver='sgd', tol=1e-15, validation_fraction=0.1, verbose=False,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build NN\n",
    "def get_rf_predictions(rfs, X):\n",
    "    nn_input_data = []\n",
    "    for klass, model in zip(CLASSES, rfs):\n",
    "        nn_input_data.append(model.predict_proba(X))\n",
    "    return np.concatenate(nn_input_data, axis=1)\n",
    "\n",
    "print('Preparing NN')\n",
    "X_nn_training = get_rf_predictions(rf_models, X_training)\n",
    "mlp = MLPClassifier(solver='sgd', activation='logistic', learning_rate='adaptive', momentum=0.9, \n",
    "              alpha=1e-6, hidden_layer_sizes=(100, 100), random_state=1, tol=1e-15)\n",
    "mlp.fit(X_nn_training, trainingDataDf[CLASSES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.115752\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "print('Preparing evaluation set')\n",
    "predictions = mlp.predict_proba(get_rf_predictions(rf_models, X_testing))\n",
    "\n",
    "# MSE\n",
    "d = predictions - testDataDf[CLASSES]\n",
    "sq_difs = map(lambda x: np.dot(x, x.T), d.as_matrix())\n",
    "print('MSE: %f' %(np.sum(sq_difs) * 1.0 / len(d)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare submission\n",
    "submissionDataDf = pd.read_csv(SUBMISSION_DATA_LOCATION)   \n",
    "\n",
    "print('Getting predictions for submission dataset')\n",
    "predictions = mlp.predict_proba(get_rf_predictions(rf_models, vectorizer.transform(submissionDataDf[TEXT_COLUMN])))\n",
    "\n",
    "y_sub = pd.DataFrame(predictions)\n",
    "y_sub.columns = CLASSES\n",
    "\n",
    "subDf = pd.concat([submissionDataDf['id'], y_sub], axis=1)\n",
    "\n",
    "print(subDf.head())\n",
    "print('Writing submissions file')\n",
    "subDf.to_csv('/home/asharma/data/toxic_challenge/submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
