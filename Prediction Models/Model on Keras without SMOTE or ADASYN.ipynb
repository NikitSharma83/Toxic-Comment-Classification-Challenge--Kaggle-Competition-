{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D, LSTM\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout\n",
    "from keras.models import Model\n",
    "from imblearn.over_sampling import SMOTE, ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the train data file\n",
    "TRAIN_LOCATION = 'C:/Users/sharm/Desktop/Dat5Melb/Final_Project/Datasets/train/stemmedtrain.csv'\n",
    "TEST_LOCATION = 'C:/Users/sharm/Desktop/Dat5Melb/Final_Project/Datasets/test1/test.csv'\n",
    "\n",
    "# utility definitions for easier handling of the dataset column names\n",
    "TEXT_COLUMN = 'comment_text'\n",
    "CLASS_TOXIC, CLASS_SEVER_TOXIC, CLASS_OBSCENE, CLASS_THREAT, CLASS_INSULT, \\\n",
    "    CLASS_IDENTITY_HATE = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \\\n",
    "                           \"insult\", \"identity_hate\"]\n",
    "CLASSES = [CLASS_TOXIC, CLASS_SEVER_TOXIC, CLASS_OBSCENE, CLASS_THREAT, CLASS_INSULT, CLASS_IDENTITY_HATE]\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 150 # average length of sentence from training set was 67\n",
    "MAX_NUM_WORDS = 200000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "# load all the data available\n",
    "dataDf_train = pd.read_csv(TRAIN_LOCATION)\n",
    "dataDf_test = pd.read_csv(TEST_LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# first, build index mapping words in the embeddings set\n",
    "# to their embedding vector\n",
    "\n",
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "with open('C:/Users/sharm/Desktop/Dat5Melb/Bias Elimination/Data/glove.6B/glove.6B.100d.txt', encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix.\n",
      "Found 204191 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix.')\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(dataDf_train[TEXT_COLUMN])\n",
    "sequences = tokenizer.texts_to_sequences(dataDf_train[TEXT_COLUMN])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training models.\n",
      "Shape of data tensor: (159571, 150)\n",
      "Shape of label tensor: (159571, 2)\n",
      "Train on 127657 samples, validate on 31914 samples\n",
      "Epoch 1/3\n",
      "127657/127657 [==============================] - 27s 211us/step - loss: 0.1525 - acc: 0.9455 - val_loss: 0.1391 - val_acc: 0.9528\n",
      "Epoch 2/3\n",
      "127657/127657 [==============================] - 25s 199us/step - loss: 0.1247 - acc: 0.9545 - val_loss: 0.1394 - val_acc: 0.9497\n",
      "Epoch 3/3\n",
      "127657/127657 [==============================] - 25s 196us/step - loss: 0.1115 - acc: 0.9587 - val_loss: 0.1257 - val_acc: 0.9546\n",
      "Shape of data tensor: (159571, 150)\n",
      "Shape of label tensor: (159571, 2)\n",
      "Train on 127657 samples, validate on 31914 samples\n",
      "Epoch 1/3\n",
      "127657/127657 [==============================] - 25s 194us/step - loss: 0.0327 - acc: 0.9890 - val_loss: 0.0318 - val_acc: 0.9892\n",
      "Epoch 2/3\n",
      "127657/127657 [==============================] - 26s 207us/step - loss: 0.0293 - acc: 0.9898 - val_loss: 0.0305 - val_acc: 0.9891\n",
      "Epoch 3/3\n",
      "127657/127657 [==============================] - 25s 194us/step - loss: 0.0270 - acc: 0.9905 - val_loss: 0.0444 - val_acc: 0.9881\n",
      "Shape of data tensor: (159571, 150)\n",
      "Shape of label tensor: (159571, 2)\n",
      "Train on 127657 samples, validate on 31914 samples\n",
      "Epoch 1/3\n",
      "127657/127657 [==============================] - 28s 220us/step - loss: 0.0856 - acc: 0.9720 - val_loss: 0.0782 - val_acc: 0.9747\n",
      "Epoch 2/3\n",
      "127657/127657 [==============================] - 22s 169us/step - loss: 0.0682 - acc: 0.9775 - val_loss: 0.0749 - val_acc: 0.9781\n",
      "Epoch 3/3\n",
      "127657/127657 [==============================] - 25s 193us/step - loss: 0.0590 - acc: 0.9800 - val_loss: 0.0767 - val_acc: 0.9762\n",
      "Shape of data tensor: (159571, 150)\n",
      "Shape of label tensor: (159571, 2)\n",
      "Train on 127657 samples, validate on 31914 samples\n",
      "Epoch 1/3\n",
      "127657/127657 [==============================] - 28s 219us/step - loss: 0.0174 - acc: 0.9965 - val_loss: 0.0129 - val_acc: 0.9971\n",
      "Epoch 2/3\n",
      "127657/127657 [==============================] - 28s 220us/step - loss: 0.0137 - acc: 0.9969 - val_loss: 0.0133 - val_acc: 0.9968\n",
      "Epoch 3/3\n",
      "127657/127657 [==============================] - 24s 189us/step - loss: 0.0124 - acc: 0.9972 - val_loss: 0.0165 - val_acc: 0.9967\n",
      "Shape of data tensor: (159571, 150)\n",
      "Shape of label tensor: (159571, 2)\n",
      "Train on 127657 samples, validate on 31914 samples\n",
      "Epoch 1/3\n",
      "127657/127657 [==============================] - 25s 192us/step - loss: 0.0965 - acc: 0.9660 - val_loss: 0.0788 - val_acc: 0.9719\n",
      "Epoch 2/3\n",
      "127657/127657 [==============================] - 23s 179us/step - loss: 0.0787 - acc: 0.9710 - val_loss: 0.0782 - val_acc: 0.9718\n",
      "Epoch 3/3\n",
      "127657/127657 [==============================] - 24s 186us/step - loss: 0.0703 - acc: 0.9740 - val_loss: 0.0892 - val_acc: 0.9713\n",
      "Shape of data tensor: (159571, 150)\n",
      "Shape of label tensor: (159571, 2)\n",
      "Train on 127657 samples, validate on 31914 samples\n",
      "Epoch 1/3\n",
      "127657/127657 [==============================] - 27s 209us/step - loss: 0.0372 - acc: 0.9911 - val_loss: 0.0299 - val_acc: 0.9913\n",
      "Epoch 2/3\n",
      "127657/127657 [==============================] - 23s 179us/step - loss: 0.0292 - acc: 0.9919 - val_loss: 0.0304 - val_acc: 0.9920\n",
      "Epoch 3/3\n",
      "127657/127657 [==============================] - 19s 150us/step - loss: 0.0251 - acc: 0.9923 - val_loss: 0.0440 - val_acc: 0.9841\n"
     ]
    }
   ],
   "source": [
    "print('Training models.')\n",
    "# http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/\n",
    "models = []\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "\n",
    "for klass in CLASSES:\n",
    "    \n",
    "    labels = to_categorical(dataDf_train.iloc[indices][klass])\n",
    "    #labels = dataDf_train.iloc[indices][klass]\n",
    "    print('Shape of data tensor:', data.shape)\n",
    "    print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "    x_train = data[:-nb_validation_samples]\n",
    "    y_train = labels[:-nb_validation_samples]\n",
    "    x_val = data[-nb_validation_samples:]\n",
    "    y_val = labels[-nb_validation_samples:]\n",
    "\n",
    "#     X_resampled, y_resampled = SMOTE().fit_sample(x_train, y_train)\n",
    "        \n",
    "    # train a 1D convnet with global maxpooling\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    preds = Dense(2, activation='softmax')(x) # set to 2 because we have 0-1\n",
    "    \n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['acc'])\n",
    "\n",
    "    #x = LSTM(50)(embedded_sequences)\n",
    "    #x = Dropout(0.5)(x)\n",
    "    #preds = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    #model = Model(sequence_input, preds)\n",
    "    #model.compile(loss='binary_crossentropy',\n",
    "    #              optimizer='sgd',\n",
    "    #              metrics=['acc'])\n",
    "    #print(model.summary())\n",
    "\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=64,\n",
    "              epochs=3, verbose=1,\n",
    "              validation_data=(x_val, y_val))\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Processing toxic\n",
      ">>> Processing severe_toxic\n",
      ">>> Processing obscene\n",
      ">>> Processing threat\n",
      ">>> Processing insult\n",
      ">>> Processing identity_hate\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    X = pad_sequences(tokenizer.texts_to_sequences(dataDf_test[TEXT_COLUMN]), \n",
    "                      maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    \n",
    "    predicted = []\n",
    "    for model, klass in zip(models, CLASSES):\n",
    "        print('>>> Processing %s' %klass)\n",
    "        predicted.append(model.predict(X)[:,1])\n",
    "        #predicted.append(model.predict(X)[:,0])\n",
    "        \n",
    "    y_sub = pd.DataFrame(np.array(predicted).T)\n",
    "    y_sub.columns = CLASSES\n",
    "\n",
    "    subDf = pd.concat([dataDf_test['id'], y_sub], axis=1)\n",
    "\n",
    "    subDf.head()\n",
    "    subDf.to_csv('C:/Users/sharm/Desktop/Dat5Melb/Final_Project/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE\n",
    "d = predictions - evaluation_classes\n",
    "sq_difs = map(lambda x: np.dot(x, x.T), d.as_matrix())\n",
    "print('MSE: %f' %(np.sum(sq_difs) * 1.0 / len(d)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 id     toxic  severe_toxic   obscene        threat    insult  \\\n",
      "0  00001cee341fdb12  0.999963  6.575979e-02  0.953947  8.918338e-09  0.925405   \n",
      "1  0000247867823ef7  0.000087  7.178182e-08  0.000022  1.434730e-09  0.000035   \n",
      "2  00013b17ad220c46  0.000306  1.293571e-11  0.000528  9.877542e-08  0.000202   \n",
      "3  00017563c3f7919a  0.000065  1.059091e-07  0.000009  2.349599e-10  0.002628   \n",
      "4  00017695ad8997eb  0.001651  2.609980e-08  0.000175  1.364719e-09  0.001154   \n",
      "\n",
      "   identity_hate  \n",
      "0   4.823716e-03  \n",
      "1   1.119048e-06  \n",
      "2   7.886510e-06  \n",
      "3   1.252631e-06  \n",
      "4   9.864416e-09  \n"
     ]
    }
   ],
   "source": [
    "print(subDf.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
